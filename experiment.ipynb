{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A0C (continuous mcts) on toy problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tree is state, state-action nodes. Works for both discrete and continuous actions\n",
    "# class StateNode:\n",
    "#   def __init__(self, state):\n",
    "#     self.state = state\n",
    "#     self.children = {} # action -> StateActionNode\n",
    "  \n",
    "#   def find_random_child(self):\n",
    "#     # in continuous case, sample from distribution\n",
    "#     # in discrete case, choose from children\n",
    "#     return None\n",
    "\n",
    "# class StateActionNode:\n",
    "#   def __init__(self, action):\n",
    "#     self.action = action\n",
    "#     self.children = {} # action -> StateActionNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy problem: move towards 10\n",
    "def step(s,a):\n",
    "  return s+a, -abs(s+a-10) # returns next state, reward\n",
    "\n",
    "def sample_action(s):\n",
    "  return np.random.uniform(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "  def __init__(self, exploration_weight=1, gamma=0.95, k=1, alpha=0.5):\n",
    "    self.gamma = gamma\n",
    "    self.exploration_weight = exploration_weight\n",
    "    self.k = k\n",
    "    self.alpha = alpha\n",
    "    self.N = defaultdict(int) # (s,a) -> visits\n",
    "    self.Q = defaultdict(float) # (s,a) -> mean value\n",
    "    self.Ns = defaultdict(int) # s -> total visits\n",
    "    self.children = defaultdict(list)\n",
    "  \n",
    "  def run(self, s, d, n_sims):\n",
    "    for _ in range(n_sims):\n",
    "      self.simulate(s, d)\n",
    "    return self.get_best_action(s)\n",
    "  \n",
    "  def simulate(self, s, d=10):\n",
    "    \"\"\"runs a MCTS simulation from state s to depth d\"\"\"\n",
    "    if d<=0:\n",
    "    #   return self.V(s) # TODO: value net\n",
    "      return 0\n",
    "\n",
    "    m = self.k * self.Ns[s] ** self.alpha # progressive widening\n",
    "    if s not in self.children or len(self.children[s]) < m:\n",
    "      a = sample_action(s)\n",
    "      self.children[s].append(a)\n",
    "    else:\n",
    "      a = self.select_action(s)                           # selection\n",
    "    s2, r = step(s, a)                                    # expansion\n",
    "    q = r + self.gamma * self.simulate(s2, d-1)           # simulation\n",
    "    self.N[(s,a)] += 1                                   \n",
    "    self.Q[(s,a)] += (q-self.Q[(s,a)])/self.N[(s,a)]      # backpropagation\n",
    "    self.Ns[s] += 1\n",
    "    return q\n",
    "  \n",
    "  def select_action(self, s):\n",
    "    \"\"\"select action based on PUCT explore/exploit\"\"\"\n",
    "    def puct(a):\n",
    "      # TODO: add policy network self.pi(a|s)\n",
    "      return self.Q[(s,a)] + self.exploration_weight * math.sqrt(self.Ns[s])/(self.N[(s,a)]+1)\n",
    "    return max(self.children[s], key=puct)\n",
    "\n",
    "  def get_best_action(self, s):\n",
    "    return max(self.children[s], key=lambda a: self.Q[(s,a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts = MCTS() # for this very simple problem we can even use gamma = 0\n",
    "mcts.simulate(0)\n",
    "\n",
    "for _ in range(300):\n",
    "  mcts.simulate(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916248534598993"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts.get_best_action(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0 action 0.916248534598993 reward -9.083751465401008\n",
      "state 0.916248534598993 action 0.9563765773781576 reward -8.12737488802285\n",
      "state 1.8726251119771506 action 0.8229031812788448 reward -7.304471706744005\n",
      "state 2.6955282932559954 action 0.982181275558391 reward -6.322290431185614\n",
      "state 3.6777095688143864 action 0.8292299095258497 reward -5.493060521659764\n",
      "state 4.506939478340236 action 0.9351034352357555 reward -4.557957086424008\n",
      "state 5.442042913575992 action 0.7622983426507535 reward -3.795658743773255\n",
      "state 6.204341256226745 action 0.8948850539934985 reward -2.900773689779756\n",
      "state 7.099226310220244 action 0.9191120484373752 reward -1.9816616413423809\n",
      "state 8.01833835865762 action 0.4064847827577298 reward -1.5751768585846513\n",
      "state 8.424823141415349 action 0.7999610502232746 reward -0.7752158083613772\n",
      "state 9.224784191638623 action 0.7347145990738773 reward -0.04050120928750012\n",
      "state 9.9594987907125 action -0.2193597512933705 reward -0.2598609605808697\n",
      "state 9.74013903941913 action 0.16523653501261082 reward -0.09462442556825934\n",
      "state 9.90537557443174 action -0.059715105951281267 reward -0.15433953151953972\n",
      "state 9.84566046848046 action 0.15545679638894527 reward -0.0011172648694053322\n",
      "state 10.001117264869405 action 0.037467303682226616 reward -0.038584568551632614\n",
      "state 10.038584568551633 action -0.2141968096557274 reward -0.17561224110409412\n",
      "state 9.824387758895906 action 0.007000267236652924 reward -0.1686119738674421\n",
      "state 9.831388026132558 action 0.12659127820085625 reward -0.042020695666586505\n",
      "state 9.957979304333413 action -0.1047694711770566 reward -0.146790166843644\n",
      "state 9.853209833156356 action 0.2143618370977487 reward -0.06757167025410382\n",
      "state 10.067571670254104 action -0.3735514869791108 reward -0.3059798167250065\n",
      "state 9.694020183274993 action 0.42177851936307076 reward -0.11579870263806491\n",
      "state 10.115798702638065 action -0.009458785130526337 reward -0.10633991750753857\n",
      "state 10.106339917507539 action 0.19190337979782224 reward -0.29824329730536014\n",
      "state 10.29824329730536 action -0.2958118289467999 reward -0.0024314683585604513\n",
      "state 10.00243146835856 action -0.0923479170645467 reward -0.0899164487059867\n",
      "state 9.910083551294013 action 0.09158960538378502 reward -0.0016731566777981044\n",
      "state 10.001673156677798 action 0.053055621223940364 reward -0.05472877790173847\n"
     ]
    }
   ],
   "source": [
    "# play a round\n",
    "s = 0 # start\n",
    "n_sims = 100\n",
    "n_moves = 30\n",
    "search_depth = 10\n",
    "\n",
    "for _ in range(n_moves):\n",
    "  best_a = mcts.run(s, search_depth, n_sims)\n",
    "  s_next, r = step(s, best_a)\n",
    "  print(\"state\", s, \"action\", best_a, \"reward\", r)\n",
    "  # print(\"ranked actions\", sorted(mcts.children[s], key=lambda a: mcts.Q[(s,a)], reverse=True))\n",
    "  s = s_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
